{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compsci 690V \n",
    "\n",
    "We pick the mini challenge 1 of VAST Challenge 2008 for this homework. \n",
    "\n",
    "**Description:** The Paraiso movement is controversial and is having considerable social impact in a specific area of the world. We have extracted a segment of the Paraiso (the movement) Wikipedia edits page. Please note this is not the Paraiso Manifesto Wiki page which is part of the background materials, but a related different page. Please use visual analytics to describe the social relationships of the editors (those that have edited/modified the Wikipedia page) as they are reflected in these files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement for MC1:\n",
    "What are the factions represented in these edit pages?  In other words, describe the individuals or groups that edit the pages with regard to any agendas and goals you hypothesize.  \n",
    "\n",
    "I parsed the text of each commit into a dataframes with the columns:\n",
    "* **timestamp** - the timestamp of the edit\n",
    "* **user** - the user or ip of the editor\n",
    "* **minorEdit** - True or False - if the edit is minor\n",
    "* **pageLength** - Length of the page after the edit (in Bytes)\n",
    "* **comment** - comment or description of the edit\n",
    "* **entireEdit** - entire raw edit text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***:\n",
    "    * Please run bokeh server\n",
    "    * jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishek\\Anaconda\\envs\\py36\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1009, 6)\n",
      "            timestamp           user minorEdit pageLength  \\\n",
      "0 2007-01-15 23:44:00         Alonzo      True     100571   \n",
      "1 2007-01-14 03:21:00        Alfonso     False     100552   \n",
      "2 2007-01-13 18:36:00        Adriano     False     102461   \n",
      "3 2007-01-13 16:45:00  DailosTamanca     False     100959   \n",
      "4 2007-01-13 15:35:00   Moisescorral     False     102461   \n",
      "\n",
      "                                          comment  \\\n",
      "0  Scientific criticism of Paraiso beliefs - link   \n",
      "1                                                   \n",
      "2                 except for eg BLP violation, et   \n",
      "3                   ; i support Gustava's changes   \n",
      "4             You propose FIRST, then you DELETE!   \n",
      "\n",
      "                                          entireEdit  \n",
      "0  # (cur) (last) 23:44, 15 January 2007 Alonzo (...  \n",
      "1  # (cur) (last) 03:21, 14 January 2007 Alfonso ...  \n",
      "2  # (cur) (last) 18:36, 13 January 2007 Adriano ...  \n",
      "3  # (cur) (last) 16:45, 13 January 2007 DailosTa...  \n",
      "4  # (cur) (last) 15:35, 13 January 2007 Moisesco...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.palettes import Spectral4\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColumnDataSource, Select,LabelSet, HoverTool\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from bokeh.models.widgets import DataTable, TableColumn\n",
    "from bokeh.layouts import Row,widgetbox\n",
    "from sklearn.manifold import MDS\n",
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Oval, ColumnDataSource,Legend,Select\n",
    "from bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool\n",
    "from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodes\n",
    "from bokeh.io import output_notebook,show,curdoc\n",
    "#output_notebook()\n",
    "\n",
    "\n",
    "file = open('VASTchallenge08-20080315-Deinosuchus/WIKI EDITS PAGE/Paraiso Edits.txt','r', errors='ignore')\n",
    "\n",
    "lines = file.readlines()\n",
    "#print (lines)\n",
    "\n",
    "df = pd.DataFrame(columns=['timestamp','user','minorEdit','pageLength','comment','entireEdit'])\n",
    "i=0\n",
    "for line in lines[3:]:\n",
    "        entireEdit = line\n",
    "        #print (entireEdit)\n",
    "        #split via brackets\n",
    "        token = re.split('\\(|\\)',line)\n",
    "        #print ('token',token)\n",
    "        timestamp = re.search('(\\d)+(.)*(\\s)(\\d\\d\\d\\d)',token[4])\n",
    "        timestamp = timestamp.group(0)\n",
    "        timestamp = pd.to_datetime(timestamp)\n",
    "        user = re.findall('(\\d\\d\\d\\d)(.+)',token[4])[0]\n",
    "        #print ('user',user)\n",
    "        user = user[1].split(' ')[1]\n",
    "        #print ('user',user)\n",
    "        m = token[6]\n",
    "        m= True if (m==' m ') else False\n",
    "        byte = token[7].split(' ')[0]\n",
    "        #print ('byte',byte)\n",
    "        try :\n",
    "            byte = int(byte.replace(',',''))\n",
    "        except:\n",
    "            byte = None\n",
    "        comment = token[-2].split('?')\n",
    "        comment = comment[-1]\n",
    "        if ('byte' in comment ):\n",
    "            comment =''\n",
    "        #print ('comment',comment)\n",
    "        \n",
    "\n",
    "        df.loc[i]=[timestamp,user,m,byte,comment,entireEdit]\n",
    "        i+=1\n",
    "\n",
    "print (df.shape)\n",
    "print (df.head())\n",
    "\n",
    "#####Thereare 1009 edits######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Term Frequency-Inverse document Frequency feature extraction and clustering\n",
    "We will use TF-idf feature extraction and cluster the users using Kmeans clustering to see if there are any distinct clusters forming. To do that, we first create a dataframe **userWiseComments** to store all the comments from all the edits of a user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 2)\n",
      "            user                                        allComments\n",
      "0  200.119.211.x                                            Origin \n",
      "1       Remedios  Reverted 1 edit by 203.59.152.x identified as ...\n"
     ]
    }
   ],
   "source": [
    "def getUserWiseComments(dataframe):\n",
    "    userWiseComments = pd.DataFrame(columns=['user','allComments'])\n",
    "    users = list(set(dataframe['user']))\n",
    "    i = 0\n",
    "    for user in users:\n",
    "        userRows = df.loc[df['user'] == user]\n",
    "        allComments = ''\n",
    "        for row in userRows.iterrows():\n",
    "            allComments += row[1]['comment'] + ' '\n",
    "        userWiseComments.loc[i] = [user,allComments]\n",
    "        i += 1\n",
    "    return userWiseComments\n",
    "\n",
    "userWiseComments = getUserWiseComments(df)\n",
    "\n",
    "print (userWiseComments.shape)\n",
    "print (userWiseComments.head(2))\n",
    "\n",
    "\n",
    "#####THERE ARE 387 DIFFERENT USERS#########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a **tfidf matrix** using the TfidfVectorizer and creating english stems of tokens for all comments of each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf matrix shape (387, 1038)\n",
      "Some features are ['join', 'jose', 'josecastro79', 'journalist', 'juan', 'juicio', 'junk', 'just', 'justic', 'justicia', 'justif', 'justifi', 'jw', 'kill', 'know', 'ko', 'ku', 'label', 'lack', 'languag', 'layman', 'lc', 'lead', 'leav', 'lectur', 'legal', 'legit', 'let', 'level', 'lie']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#take comments from dataframe\n",
    "commentsDf = userWiseComments['allComments']\n",
    "\n",
    "#token given text and find their stems\n",
    "def tokenizeAndStem(text):\n",
    "    CommentsTokens=[]\n",
    "    #for userComments in commentsDf:\n",
    "    CommentsTokens = (nltk.word_tokenize(text))\n",
    "    #filter out punctuations and numeric tokens\n",
    "    filtered_tokens = []\n",
    "    for token in CommentsTokens:\n",
    "        if re.search('[a-zA-Z0-9]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "#making Term Frequency-Inverse document Frequency matrix model\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8,\n",
    "                                 min_df=0, stop_words='english',\n",
    "                                 use_idf=False,tokenizer=tokenizeAndStem, ngram_range=(1,1))  #creating features by taking 3 words at a time\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(commentsDf)\n",
    "print ('tf-idf matrix shape',tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "#terms are list of features in the matrix\n",
    "print ('Some features are',terms[500:530])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create **clusters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "def get_colors(clusters):\n",
    "    colors=[]\n",
    "    for i in clusters:\n",
    "        if i==0:\n",
    "            colors.append('red')\n",
    "        elif i==1:\n",
    "            colors.append('blue')\n",
    "        elif i==2:\n",
    "            colors.append('green')\n",
    "        elif i==3:\n",
    "            colors.append('yellow')\n",
    "    return colors\n",
    "\n",
    "colors = get_colors(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the **top 10 features** of each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['version', 'agustin', 'jibbon7', 'hamsterlopithecus', 'hispa', 'sara', 'honoria', 'snakey', 'rosalind', 'socorro']\n",
      "['paraiso', 'belief', 'critic', 'link', 'page', 'practic', 'influenc', 'controversi', 'ad', 'replac']\n",
      "['origin', 'hous', 'mind', 'definit', 'lectur', 'text', 'influenc', 'belief', 'error', 'especi']\n",
      "['revert', 'revis', 'vandal', 'edit', 'use', '1', 'tw', 'identifi', 'xxxxxxxxx', 'edemir']\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:,::-1]\n",
    "topFeatures=[]\n",
    "for centres in order_centroids:\n",
    "    topFeature =[]\n",
    "    for words in centres[:10]:\n",
    "        topFeature.append(terms[words])\n",
    "    topFeatures.append(topFeature)\n",
    "    print (topFeature)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, we focus on Visualization:\n",
    "This step requires **Dimension Reduction**\n",
    "\n",
    "We can try and test 3 dimension reduction algos:\n",
    " 1. MDS - Multidimensional scaling (MDS) is a set of data analysis techniques that display the structure of distance-like data as a geometrical picture. [More Info]( http://www.benfrederickson.com/multidimensional-scaling) \n",
    " 2. PCA - We do orthonormal linear transformation such that data variance is maximized. It can be thought as special case of SVD in fact.\n",
    " 3. Truncated SVD -  singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. The original matrix is sparse and we find 'concept' related to each cluster. [More information]( https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PCAresult = PCA(n_components=2).fit_transform(tfidf_matrix.toarray())\n",
    "xs = PCAresult[:,0]\n",
    "ys = PCAresult[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
    "svdResult = svd.fit_transform(tfidf_matrix)  \n",
    "\n",
    "xs  = svdResult[:,0]\n",
    "ys = svdResult[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dimensionality reduction for the TF-IDF matrix (or df equally)\n",
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, random_state=1)\n",
    "\n",
    "#toarray() converts sparse array to dense numpy array\n",
    "pos = mds.fit_transform(tfidf_matrix.toarray())  # shape (n_components, n_samples)\n",
    "\n",
    "#store the dimensions in xs, ys\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tables for Graph rendering.\n",
    "   * Posx and PosY are given by Dimension reduction algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 4)\n",
      "            user      posX      posY  cluster\n",
      "0  200.119.211.x  0.929167  0.200235        2\n",
      "1       Remedios  0.030112  0.962936        3\n",
      "2        Eltri85  0.013001  0.006355        1\n",
      "3         Aluino  0.989880 -0.194505        1\n",
      "4       Clodoveo -0.096577 -0.762346        1\n"
     ]
    }
   ],
   "source": [
    "userList  = list(userWiseComments['user'])\n",
    "\n",
    "def update_userNetworkDf(df,xs,ys,clusters):\n",
    "    \n",
    "    userNetworkDf = pd.DataFrame(columns = ['user','posX','posY'])\n",
    "    userNetworkDf['user'] = userList\n",
    "    userNetworkDf['posX']  = xs\n",
    "    userNetworkDf['posY']= ys\n",
    "    userNetworkDf['cluster'] = clusters\n",
    "    return userNetworkDf\n",
    "\n",
    "userNetworkDf = update_userNetworkDf(df,xs,ys,clusters)\n",
    "\n",
    "print (userNetworkDf.shape)\n",
    "print (userNetworkDf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#supporing data structure for legends\n",
    "\n",
    "def updateSources(userNetworkDf):\n",
    "    sourceMat=[]\n",
    "    for name,group in userNetworkDf.groupby('cluster'):\n",
    "        sourceMat.append(group)\n",
    "    return sourceMat\n",
    "\n",
    "sourceMat = updateSources(userNetworkDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 4)\n",
      "  from   to       fromName        toName\n",
      "0   73  176  DailosTamanca       Gustava\n",
      "1  176    7        Gustava  Moisescorral\n",
      "2  344  263         Edemir        Dragon\n",
      "3  176   99        Gustava    61.9.148.x\n",
      "4   12  127      Salvatora       Honoria\n"
     ]
    }
   ],
   "source": [
    "edges = pd.DataFrame(columns=['from','to','fromName','toName'])\n",
    "\n",
    "for row in df.iterrows():\n",
    "    \n",
    "    for name in userList:\n",
    "        if name in row[1]['comment'] and name!=' ' and name!=row[1]['user']:\n",
    "            fromName = row[1]['user']\n",
    "            toName = name\n",
    "            edgefrom = userNetworkDf[userNetworkDf['user'] ==row[1]['user'] ].index.tolist()[0]\n",
    "            edgeto = userNetworkDf[ userNetworkDf['user'] ==name ].index.tolist()[0]\n",
    "            edges.loc[len(edges)] = [edgefrom,edgeto,fromName,toName]\n",
    "            \n",
    "print (edges.shape)\n",
    "print (edges.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for Plotting the Graph.**\n",
    "  * Nodes are given by the users who have edited the Wikipedia Page\n",
    "  * Coloring scheme is derived from Clustering\n",
    "  * An edge from one node to other represent any mention/reverts done by that user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E-1001 (BAD_COLUMN_NAME): Glyph refers to nonexistent column name: fill_color [renderer: GlyphRenderer(id='5b12d53a-3108-470b-b055-167cc3276594', ...)]\n"
     ]
    }
   ],
   "source": [
    "#plot\n",
    "plot = figure(title=\"Graph of Wiki Users\",\n",
    "              height=700,width=1200, tools=[\"tap\",'pan','wheel_zoom','box_zoom','reset','box_select'],\n",
    "              x_range = (-1.1,1.1),\n",
    "              y_range=(-1.1,2.6))\n",
    "\n",
    "\n",
    "#legend code\n",
    "source0 = ColumnDataSource(data=dict(x=sourceMat[0]['posX'],y=sourceMat[0]['posY'], \n",
    "                                                                    user = list(sourceMat[0]['user'])))\n",
    "source1 = ColumnDataSource(data=dict(x=sourceMat[1]['posX'],y=sourceMat[1]['posY'], \n",
    "                                                                    user = list(sourceMat[1]['user'])))\n",
    "source2 = ColumnDataSource(data=dict(x=sourceMat[2]['posX'],y=sourceMat[2]['posY'], \n",
    "                                                                    user = list(sourceMat[2]['user'])))\n",
    "source3 = ColumnDataSource(data=dict(x=sourceMat[3]['posX'],y=sourceMat[3]['posY'], \n",
    "                                                                    user = list(sourceMat[3]['user'])))\n",
    "\n",
    "c0 = plot.circle(x='x',y='y',source = source0,size=2,alpha=0.5,fill_color='red',line_color='red',legend = str(topFeatures[0]))\n",
    "c1 = plot.circle(x='x',y='y',source=source1,size=2,alpha=0.5,fill_color='blue',line_color='blue',legend = str(topFeatures[1]))\n",
    "c2 = plot.circle(x='x',y='y',source= source2,size=2,alpha=0.5,fill_color='green',line_color='green',legend = str(topFeatures[2]))\n",
    "c3 = plot.circle(x='x',y='y',source=source3,size=2,alpha=0.5,fill_color='yellow',line_color='yellow',legend = str(topFeatures[3]))\n",
    "\n",
    "\n",
    "\n",
    "########graph#########\n",
    "graph = GraphRenderer()\n",
    "Node  =  graph.node_renderer\n",
    "\n",
    "########nodes##########\n",
    "graph.node_renderer.glyph = Circle(size=7, fill_color='fill_color')\n",
    "graph.node_renderer.selection_glyph = Circle(size=7, fill_color=Spectral4[2])\n",
    "graph.node_renderer.nonselection_glyph = Circle(size=7, fill_color='fill_color',fill_alpha=0.1,line_alpha=0.1)\n",
    "graph.node_renderer.hover_glyph = Circle(size=7, fill_color=Spectral4[1])\n",
    "\n",
    "graph.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.8, line_width=1)\n",
    "graph.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=1)\n",
    "graph.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=5)\n",
    "\n",
    "\n",
    "#data\n",
    "node_indices = list(range(0,len(userNetworkDf)))\n",
    "Node.data_source.data = dict(index=node_indices,fill_color = colors,user = userNetworkDf['user'])\n",
    "graph.edge_renderer.data_source.data = dict(start=list(edges['from']),end=list(edges['to']),\n",
    "                                           startName = list(edges['fromName']),endName = list(edges['toName']))\n",
    "\n",
    "\n",
    "### start of layout code\n",
    "x = list(userNetworkDf['posX'])\n",
    "y=list(userNetworkDf['posY'])\n",
    "graph_layout = dict(zip(node_indices, zip(x, y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "edge_hover_tool = HoverTool( tooltips=[('revert/mentions',\"@startName -> @endName\")])\n",
    "plot.add_tools(edge_hover_tool)\n",
    "#c = plot.circle(x='x',y='y',source=source,size=15,alpha=0.001,fill_color='color')\n",
    "plot.legend.label_text_font_size ='7pt'\n",
    "plot.legend.location = 'top_left'\n",
    "plot.legend.background_fill_alpha = 0.1\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "#interaction policies\n",
    "graph.inspection_policy = EdgesAndLinkedNodes()\n",
    "graph.selection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "\n",
    "show(plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for intercation and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Red['version', 'agustin', 'jibbon7', 'hamsterlopithecus', 'hispa', 'sara', 'honoria', 'snakey', 'rosalind', 'socorro'] \n",
      " Blue['paraiso', 'belief', 'critic', 'link', 'page', 'practic', 'influenc', 'controversi', 'ad', 'replac']\n",
      " Green['origin', 'hous', 'mind', 'definit', 'lectur', 'text', 'influenc', 'belief', 'error', 'especi']\n",
      " Yellow['revert', 'revis', 'vandal', 'edit', 'use', '1', 'tw', 'identifi', 'xxxxxxxxx', 'edemir']\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "from bokeh.layouts import row, layout\n",
    "DimReductionList =['MDS',\n",
    "                  'PCA',\n",
    "                  'Truncated SVD']\n",
    "\n",
    "#widget\n",
    "algo_attribute_select = Select(value='Truncated SVD',\n",
    "                          title='Select algo for Dimension Reduction:',\n",
    "                          width=200,\n",
    "                          options=DimReductionList)\n",
    "\n",
    "\n",
    "\n",
    "#callback\n",
    "def update_algo_attribute(attrname,old,new):\n",
    "    algo = algo_attribute_select.value\n",
    "    print ('in the func')\n",
    "    if algo=='MDS':\n",
    "        mds = MDS(n_components=2, random_state=1)\n",
    "        pos = mds.fit_transform(tfidf_matrix.toarray())  # shape (n_components, n_samples)\n",
    "        xs, ys = pos[:, 0], pos[:, 1]\n",
    "        #print (xs,ys)\n",
    "    if algo=='PCA':\n",
    "        PCAresult = PCA(n_components=2).fit_transform(tfidf_matrix.toarray())\n",
    "        xs = PCAresult[:,0]\n",
    "        ys = PCAresult[:,1]\n",
    "        #print (xs,ys)\n",
    "    if algo=='TSNE':\n",
    "        TSNEresult = TSNE(n_components=2).fit_transform(tfidf_matrix.toarray())\n",
    "        xs = TSNEresult[:,0]\n",
    "        ys = TSNEresult[:,1]\n",
    "        #print (xs,ys)\n",
    "    if algo=='Truncated SVD':\n",
    "        svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
    "        svdResult = svd.fit_transform(tfidf_matrix)\n",
    "        xs  = svdResult[:,0]\n",
    "        ys = svdResult[:,1]\n",
    "        #print (xs,ys)\n",
    "    \n",
    "    userNetworkDf = update_userNetworkDf(df,xs,ys,clusters) \n",
    "    print ('userNteworkDf updated..')\n",
    "    sourceMat =updateSources(userNetworkDf)\n",
    "    \n",
    "    #data\n",
    "    node_indices = list(range(0,len(userNetworkDf)))\n",
    "    Node.data_source.data = dict(index=node_indices,fill_color = colors,user = userNetworkDf['user'])\n",
    "\n",
    "\n",
    "    ### start of layout code\n",
    "    x = list(userNetworkDf['posX'])\n",
    "    y=list(userNetworkDf['posY'])\n",
    "    graph_layout = dict(zip(node_indices, zip(x, y)))\n",
    "    graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "    plot.renderers.append(graph)\n",
    "    \n",
    "    source0.data = dict(x=sourceMat[0]['posX'],y=sourceMat[0]['posY'], \n",
    "                                                                    user = list(sourceMat[0]['user']))\n",
    "    source1.data = dict(x=sourceMat[1]['posX'],y=sourceMat[1]['posY'], \n",
    "                                                                        user = list(sourceMat[1]['user']))\n",
    "    source2.data = dict(x=sourceMat[2]['posX'],y=sourceMat[2]['posY'], \n",
    "                                                                        user = list(sourceMat[2]['user']))\n",
    "    source3.data = dict(x=sourceMat[3]['posX'],y=sourceMat[3]['posY'], \n",
    "                                                                        user = list(sourceMat[3]['user']))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print ('\\n Red'+str(topFeatures[0])+' \\n Blue'+str(topFeatures[1])+'\\n Green'+str(topFeatures[2])+ \n",
    "               '\\n Yellow'+ str(topFeatures[3]))\n",
    "   \n",
    "\n",
    "algo_attribute_select.on_change('value', update_algo_attribute)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few users that stand out in group ['belief', 'use', 'vandal', 'revert', 'revis', 'link', 'edit', 'practic', 'critic', 'influenc'] are:-\n",
    "\n",
    "* VictoriaV\n",
    "* RyogaNica\n",
    "* DailosTamanca\n",
    "* Rm99\n",
    "\n",
    "In another group ['paraiso', 'religion', 'replac', 'page', 'critic', 'belief', 'cult', 'scientif', 'celebr', \"'s\"]:\n",
    "* Sara\n",
    "* Edemir\n",
    "* Augustin\n",
    "* Amado\n",
    "\n",
    "\n",
    "To be noted:\n",
    "- Sara and VictoriaV have edited/revertes DailosTamanca. So possibly they are together. There is no other user in graph which have common reverts. \n",
    "- VictoriaV is clearly opposite to Rm99 and Augustin. Sara has edges to Rm99. This possibly confirm hypothesis in above statement that VictoriaV and Sara are together.\n",
    "- We cannot judge whose side is Edimir on. He has most edges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Creating groups via reverts and mentions\n",
    "\n",
    "In this method we hypothesize that the users will be split into 4 major group:\n",
    "1. Bots.\n",
    "2. Neutral users (unbiased).\n",
    "3. Users aligned with Government of India, Indian Army and Central Reserve Police.\n",
    "4. Users aligned with Kashmiri protesters and separatists.\n",
    "\n",
    "Wikipedia bots have Bot at the end of their name, therefore getting group one is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userWiseComments2 = getUserWiseComments(df)\n",
    "userMentions = pd.DataFrame(columns=['user','mention','phrase', 'to'])\n",
    "listOfUserNames = list(userWiseComments2['user'])\n",
    "#sia = SentimentIntensityAnalyzer()\n",
    "for row in userWiseComments2.iterrows():\n",
    "    tokens = row[1]['allComments'].split()\n",
    "    for name in listOfUserNames:\n",
    "        indexes = [i for i,token in enumerate(tokens) if token==name]\n",
    "        for i in indexes:\n",
    "            start = max(0, i-5)\n",
    "            phrase = \" \".join(tokens[start:i+1])\n",
    "            printText = \"\"\n",
    "            to = False\n",
    "            for word in tokens[start:i+1]:\n",
    "                if(word == 'to' or word == 'To'):\n",
    "                    printText +=word\n",
    "                    to = True\n",
    "                else:\n",
    "                    printText +=word + \" \"\n",
    "            #print(printText)\n",
    "            userMentions.loc[len(userMentions)] = [row[1]['user'],name,phrase,to]\n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relationshipMatrix = {}\n",
    "for name1 in userList:\n",
    "    relationshipMatrix[name1] = {}\n",
    "    for name2 in userList:\n",
    "        relationshipMatrix[name1][name2] = 0\n",
    "        \n",
    "for row in userMentions.iterrows():\n",
    "    if(row[1]['to'] == True):\n",
    "        if(row[1]['user']<row[1]['mention']):\n",
    "            relationshipMatrix[row[1]['user']][row[1]['mention']] += 1\n",
    "        else:\n",
    "            relationshipMatrix[row[1]['mention']][row[1]['user']] += 1\n",
    "    else:\n",
    "        if(row[1]['user']<row[1]['mention']):\n",
    "            relationshipMatrix[row[1]['user']][row[1]['mention']] -= 1\n",
    "        else:\n",
    "            relationshipMatrix[row[1]['mention']][row[1]['user']] -= 1\n",
    "\n",
    "relationships = pd.DataFrame(columns=['user1','user2','relationship'])\n",
    "i=0\n",
    "for key1,dictionary in relationshipMatrix.items():\n",
    "        for key2,value in dictionary.items():\n",
    "            if(value != 0):\n",
    "                relationships.loc[i] = [key1,key2,value]\n",
    "                i+=1\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heatmap is created based on relationship matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E-1001 (BAD_COLUMN_NAME): Glyph refers to nonexistent column name: fill_color [renderer: GlyphRenderer(id='5b12d53a-3108-470b-b055-167cc3276594', ...)]\n"
     ]
    }
   ],
   "source": [
    "from bokeh.models import (\n",
    "    ColumnDataSource,\n",
    "    HoverTool,\n",
    "    LinearColorMapper,\n",
    "    BasicTicker,\n",
    "    PrintfTickFormatter,\n",
    "    ColorBar,\n",
    ")\n",
    "from math import pi\n",
    "import pandas as pd\n",
    "from bokeh.palettes import Spectral6\n",
    "from bokeh.io import show\n",
    "\n",
    "relationdf = pd.DataFrame.from_dict(data=relationshipMatrix)\n",
    "#print (relationdf.head())\n",
    "\n",
    "\n",
    "relationdf.index.name = 'user1'\n",
    "relationdf.columns.name = 'user2'\n",
    "\n",
    "user1 = list(relationdf.index)\n",
    "user2 = list(relationdf.columns)\n",
    "\n",
    "df2 = pd.DataFrame(relationdf.stack(), columns=['coeff']).reset_index()\n",
    "#colors = [ \"#a5bab7\", \"#c9d9d3\", \"#e2e2e2\", \"#dfccce\", \"#ddb7b1\", \"#cc7878\", \"#933b41\"]\n",
    "colorsHeatMap = [ \"#a5bab7\", \"#ddb7b1\", \"#dfccce\",\"#933b41\"]\n",
    "colorsHeatMap = list(reversed(colorsHeatMap))\n",
    "mapper = LinearColorMapper(palette=colorsHeatMap, low=df2.coeff.min(), high=df2.coeff.max())\n",
    "source = ColumnDataSource(df2)\n",
    "\n",
    "TOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\"\n",
    "\n",
    "p = figure(title=\"relationship mat\",\n",
    "           x_range=user1, y_range=user2,\n",
    "           x_axis_location=\"above\", plot_width=1000, plot_height=700,\n",
    "           tools=TOOLS, toolbar_location='below')\n",
    "\n",
    "p.grid.grid_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.axis.major_label_text_font_size = \"3pt\"\n",
    "p.axis.major_label_standoff = 1\n",
    "p.xaxis.major_label_orientation = pi / 3\n",
    "p.yaxis.major_label_orientation = pi / 3\n",
    "\n",
    "p.rect(x=\"user1\", y=\"user2\", width=1, height=1,\n",
    "       source=source,\n",
    "       fill_color={'field': 'coeff', 'transform': mapper},\n",
    "       line_color=None)\n",
    "\n",
    "color_bar = ColorBar(color_mapper=mapper, major_label_text_font_size=\"5pt\",\n",
    "                     ticker=BasicTicker(desired_num_ticks=len(colorsHeatMap)),\n",
    "                     formatter=PrintfTickFormatter(format=\"%d%%\"),\n",
    "                     label_standoff=6, border_line_color=None, location=(0, 0))\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "p.select_one(HoverTool).tooltips = [\n",
    "     ('compare', '@user2 -> @user1'),\n",
    "     ('score', '@coeff'),\n",
    "]\n",
    "\n",
    "show(p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation with Regard to HW5 and 6:**\n",
    "    * We can see that Truncated SVD and PCA gives fairly good idea about clusters. MDS dimension reduction is not good enough.\n",
    "    * Method 1 is very good to find 'Edit-wars' in Wikipedia, i.e. finding two factions and their ideology.\n",
    "    * We can see that the group that has features such as 'reverts' or 'edits' are opposite to another group. One faction can be seen as aligned to Indian state and another to rebels/Pakistan.\n",
    "    * Relationship matrix re-interates the memebers of a particular group. Heat map has 289*289 cells so it is difficult to visualize on small screens. It gives a very good idea about a specific user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation with Regard to HW7:**\n",
    "    * We can see that Truncated SVD gives poor results this time. Many nodes overlap each other. MDS and PCA are distributed, so we can inspect easily.\n",
    "    * Many user with edits who do not have edges, are possibly in nuetral group.\n",
    "    * There is central node in MDS which includes many bots, which has several user overlapped. It is therefore useless for inspection.\n",
    "    * There is a cluster which contain features such as ['belief', 'version', 'vandal', 'use', 'revert', 'revis', 'link', 'edit', 'practic', 'critic']. It may contain both the factions in edit-war:pro-paraiso and anti-paraiso.\n",
    "    * The heat map is useless here as compared to previous homework. Does not give any good information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using obeservations of Graph*\n",
    "\n",
    "Lets look at our predictions, and analyse the situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reverted good faith edits by Dragon; No need to dampen it; the reference points to a blanket opinion of psychiatry and psychology.. using TW  - Counselor doesn\\'t need quotes here. Reverted 1 edit by 24.195.147.x identified as vandalism to last revision by Diegoob. using TW  Paraiso as a cult - More cleanup, more typos, fixed refs, etc. Texts and Lectures - More vandalism cleanup, and moved a period Origin - Trimming out more vandalism .  - Removing vandalism Minor language - Good job on that pernicious sentence, Amado That darn sentence... I\\'m not even sure the intro should be this long. Restored language that actually does reflect the contents of the refs, and removed a ref that didn\\'t apply.  Neutral, factually accurate. Says nothing more or less than that the State Department has such reports, and that Paraisos report descrimination. Previous phrasing is more neutral, but the ref is fine. Introduction - Minor language Reverted 2 edits by Alverio identified as vandalism to last revision by Rufina. using TW Reverted 1 edit by Alberto identified as vandalism to last revision by Sara. using TW Reverted good faith edits by Callas; That more properly belongs in the Angel article, where it is already.. using TW Actually, that\\'s not a typo, it\\'s really spelled that way: Deseret Membership - Being bold, removing inaccurate image map per talk page. Influences - Language cleanup, references cleanup  - Undoing because the added ref isn\\'t a RS, and the new language wasn\\'t grammatical. Training - Cleaned up grammar and syntax, rephrased for readability, replaced \"smart\" punctuation, formatted refs, etc... Undid revision xxxxxxxxx - VictoriaV, it\\'s still not relevant, and we\\'ve no reason to doubt the source. See talk page. . using [[W Membership - Spotted another one. Membership - Cleaned up some typos around refs  The rest is superfluous, but the actual number of interviewees is fine -- and more importantly, in the ref. Membership - That\\'s OR, as the ref doesn\\'t mention that percentage. Besides, that\\'s a perfectly valid sample size for statistical analysis. Reverted 1 edit by 207.61.57.x identified as vandalism to last revision by Seina. using TW Reverted 1 edit by 209.250.162.x identified as vandalism to last revision by Edemir. using TW Reverted to revision xxxxxxxxx by Niermague; Restoring unvandalized version - Anoryat removed punctuation unnecessarily. using TW Reverted 1 edit by 75.131.224.x identified as vandalism to last revision by Alvaro. using TW Reverted 1 edit by 58.179.241.x identified as vandalism to last revision by BakBOT. using TW Beliefs - Tone reads like an advertisement for the church in some places. Removed extraneous cruft that got re-added in an anti-vandalism revert Typos Paraiso as a state-recognized religion - If references for these statements aren\\'t found post-haste, this paragraph goes. Reverted 1 edit by Oscar identified as vandalism to last revision by ErKomandante. using TW Home Health Care - added confrontation of Paraiso members and Dept of Health Home Health Care - Dept of Health intervention refs Home schooling for girls - Whoops, messed up the punctuation. Sorry.   Home schooling for girls - Language and syntax cleanup, removed some redundancy, wikilinked \"home schooling\" etc. Not disputing these claims, but they need references, and I\\'m not sure the intro is the right place for this. Home schooling for girls - Removed extraneous carriage-return  - Unsourced and redundant POV addition  Scientific criticism of Paraiso\\'s beliefs - Removed vandalism  - Vandalism Corrected typo ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ((userWiseComments[userWiseComments['user']=='Edemir']['allComments'].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From graph, Edimir has notably, edges to:**\n",
    "- VictoriaV\n",
    "- Sara\n",
    "- Amado\n",
    "\n",
    "**Few Highlights of Edemir edit**\n",
    "- Reverted 1 edit by Alberto identified as vandalism to last revision by Sara. \n",
    "- Undid revision xxxxxxxxx - VictoriaV, it\\'s still not relevant\n",
    "- addition  Scientific criticism of Paraiso\\'s beliefs\n",
    "- revert Typos Paraiso as a state-recognized religion - If references for these statements aren\\'t found post-haste, this paragraph goes.\n",
    "- **using TW Home Health Care - added confrontation of Paraiso members and Dept of Health Home Health Care - Dept of Health intervention**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Morals and Ethics Organizations - moved pics around Influences Paraiso Justice - This whole section is false, there is no such info in Paraiso Ethics Book, I did look! I was going to edit but this Justice is of too little importance to be here. Paraiso Ethics - moved a sentence around The Cielo - wikilink Restored Paraiso Paraiso basic concepts while maintaining \"well sourced materials\" know you don\\'t have an excuse to undue me Paraiso Ethics - The states of existance Ethics in Paraiso Ethics in Paraiso Criticisms of Paraiso Ethics Paraiso Justice Deleted false statement. A person can only be declared afther it has been proven in a Justicia Juicio that he commited a high crime. Intro to C. Ethics 1998 is no longer used refer to 2006 edition only Morals Ethics in Paraiso - Created subsection for Criticisms of Paraiso Ethics The Cielo Right and Wrong morals Survive and Right and Wrong New: Right and wrong  Removed pov pushing from first sentence, the controversy is already covered in the intro there is no need to push it in the 1st sentence Undid revision xxxxxxxxx by Amado Sorry I reverted the robot by mistake my bad. Undid revision xxxxxxxxx by CristovalSorry, That is POV pushing by putting that in the 1st sentence. Paraiso as a state-recognized religion /* Paraiso gains religious recognition in Spain Auditing Confidentiality - moved a Pablo6213grph around Membership - moved picture down a little for looks Controversy and criticism - moved one paragraph around Membership /* Paraiso Training Beliefs - removed duplicate information already in page Beliefs Beliefs Beliefs Beliefs Beliefs Beliefs Beliefs - the goal of Paraiso Reworded intro, same info different wording. Made intro short, sweet and NPOV. The intro should stay NPOV with no specifics of why or whom. The body of the info is already in the body of this page. Mode intro less POV The US state department has supported Paraiso\\'s quest for religious freedom. added citations All issues have two sides, here is the other side of the coin. Beliefs - Created subsection using existing paragraph / the bridge to total freedom The Hombre / added quote The Alma - added quote Influences - added wikilink Influences - added citation Influences - added more info on influences Definition - Re-aranged section with the more aplicable info in front. Influences Influences Influences - more info Influences - added citation Definition - moved definition as used in Paraiso to the first line Influences - added influence by Vidros. Influences - removed uncited opinion, F did mention his influences in plenty of times, specialy during the Miami lectures Origin - Divided origin into two sections Origin and Influences because these are two different subjects  Levels 0 - 5 Definition - explanation in accordance with talk page Moved levels under auditing Beliefs - added \"Paraiso Manifesto\" Beliefs - added the parts of man Beliefs - Simplified beliefs Stop your POV pushing Rm99!!!! More POV pushing in the lead, one sided. Stop it!!!!  futher simplify intro added texts and lectures Origin - added sentence more organization Priciples Defintion Created main page \"Paraiso Definition\", replaced contents of \"Paraiso Definition\" for shorter version. reorganized page a little added sentence to intro to facilitate understanding added citacion There is no citation that Paraiso also refers to the Paraiso Movement. Paraiso is Paraiso the Movement is the Movement, lets differentiate. There is no need to take sides in the intro. The fact that Paraiso is crontroversial is good enoght there are also Journalists, courts and the governing bodies that are not critical of Paraiso Paraiso and other religions Paraiso and other religions - added citation ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ((userWiseComments[userWiseComments['user']=='Amado']['allComments'].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From graph, Amado has reverted/mentioned:**\n",
    "- Rm99\n",
    "\n",
    "**Sara, Edemir, VictoriaV, RyogaNica have reverted edits by Amado**\n",
    "\n",
    "**Highlights from Amado edits:**\n",
    "- there are also Journalists, courts and the governing bodies that are not critical of Paraiso\n",
    "- Influences - removed uncited opinion\n",
    "- Stop your POV pushing Rm99!!!!\n",
    "- Paraiso Justice Deleted false statement\n",
    "\n",
    "It seems Amado is pro-Paraisan, in group with VictoriaV and Sara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WEASEL word, there is no dispute here  Influences - read the ref, there goes the propaganda once more. Influences - missing fact added Paraiso as a state-recognized religion Reverted 1 edit by Edemir; See, GoodD, you actually should read the refs. makes more sense then. . using TW Reverted 1 edit by Edemir; \"listed\" is factually wrong. each country gets its own report.. using TW added ref and adjusted text  non-RS middle thing it\\'s a bit stronger, man. porn link farm removed typo try google site:state.gov +Paraiso before you kill text. just same samples found in 30sec. non-RS needs to be exchanged with real ref original source for ref included  Demography of the United States  anyway, i don\\'t insist on the %s but the pop is in the ref.  well, ok, i used a calculator. easy on WP:V  Membership - dunno the methodic reqs but this % is really low, isn\\'t it. another one code stuff \". Reverted to revision xxxxxxxxx by Mandos; unreliable source cannot be used. using TW Reverted to revision xxxxxxxxx by Amado; you. are. trying. to. violate. WP:NPOV. no. way.. using TW yawn, POV pushing Paraiso as a state-recognized religion - phase two, adding ref Reverted to revision xxxxxxxxx by VictoriaV; phase one, restoring arbitrary deletion. . using TW Undid revision xxxxxxxxx by Agustin If you don\\'t understand something, ask, but don\\'t delete references. External links - primary information section updated Reverted to revision xxxxxxxxx by VictoriaV; back to where we started off. this is ridiculous. I still invite you to go to the talk page and bring ARGUMENTS!. using TW Reverted to revision xxxxxxxxx by VictoriaV; Rm99, also all \"positive\" secondary ones, but I didn\\'t expect you to see that. . using TW Auditing Confidentiality - sheffield, please stick to accurate descriptions. you just faked the quote but \"improving the presentation\". Great idea, Agustin, if that settles the matter, I am all for it. Reverted to revision xxxxxxxxx by Rm99; bummer, three reverts and more for Rm99 again. Let\\'s freeze this site once more. . using TW Reverted 1 edit by Rm99 identified as vandalism to last revision by VictoriaV. using TW ok, Hartley, 200 it is Reverted 1 edit by Rm99 identified as vandalism to last revision by VictoriaV. using TW Reverted 1 edit by Eunispero to last revision by VictoriaV; Sorry, mister, but it is a WP violating ANONYMOUS ra-ra blog of alleged interviews. anyone can edit this.. using [[WP:TWINKLE Commercial book promotion moved in EL section. Paraiso as a cult - see talk page Free Zone links - dead link removed Undid revision xxxxxxxxx. Go ahead, but stop destroying my edits. Undid revision xxxxxxxxx. Sorry, this is POV-pushing an unacceptable. Most of those ELs are violating WP:EL and you know it. Don\\'t try to cover it up. Reverted 1 edit by DailosTamanca to last revision by VictoriaV; Show me a GOVERNMENT who said that. this is intentionally worded that way.. using TW Reverted to revision xxxxxxxxx by VictoriaV; the usual crap by Rm99. using TW Reverted 1 edit by Rm99 identified as vandalism to last revision by VictoriaV. using TW Reverted 2 edits by Rm99 to last revision by Agustin; POV pushing, rv attempt to whitewash private hate sites. using TW Private hobby pages are non-RS, Personal critics\\' pages quoting personal critics are even worse. I think there are other refs for this sentence. Put them there. This is Raymond Shores\\'s anti-P page quoting private anti-sect fanatic Jose Contrera, incorrectly used as ref. removed as incompatible w WP:RS and tagged Other links  Critical links - let\\'s get this in perspective here. Reverted to revision xxxxxxxxx by Savanna; private hobby sites rved. using TW ']\n"
     ]
    }
   ],
   "source": [
    "print ((userWiseComments[userWiseComments['user']=='VictoriaV']['allComments'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From graph, VictoriaV has notably, edges to:**\n",
    "- Edimir\n",
    "- Rm99\n",
    "- Agustin\n",
    "- DailosTamanca\n",
    "- Amado\n",
    "\n",
    "**Highlights from VictoriaV edit**\n",
    "- added Paraiso as a state-recognized religion\n",
    "- Undid revision xxxxxxxxx by Agustin If you don\\'t understand something, ask, but don\\'t delete references\n",
    "- Reverted 1 edit by Rm99 identified as vandalism to last revision by VictoriaV\n",
    "- **There are lot of instances against Augustin and Rm99.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Obervations**\n",
    "- VictoriaV states Paraiso as state-recognized religion. HE/she is Pro-paraiso. Sara and Amado are teamed up also.\n",
    "- In other group are- Agustin and Edemir and Rm99.\n",
    "- There is lot of mention of 'vandalism' and words like 'unvandalised'.It even features in top 10 features of 2 clusters. \n",
    "Particularly by Edemir and VictoriaV.\n",
    "- Since Edemir is anti-Paraiso, he states ''using TW Home Health Care - added confrontation of Paraiso members and Dept of Health Home Health Care - Dept of Health intervention''. **This possibly suggests that a few elements of movements might include violence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to be noted**\n",
    "While arriving at solution, visualization was used as a tool to help. And not give final answer. Although \n",
    "clustering gave a very good idea about groups and faction, an assumption was marked while making graph.\n",
    "This is, that all mentions/reverts in the comments by user of another user are in opposite/negative sense. It helped chalk out and visualise the edges. On further inspecting, could I finally conclude the possibilities.\n",
    "\n",
    "**The answer arrived was already given but visualization was a bit different. Had the answer been not known, most of the people might have been grouped correctly by Visual Inspection only. Only confusion was Edemir.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E-1001 (BAD_COLUMN_NAME): Glyph refers to nonexistent column name: fill_color [renderer: GlyphRenderer(id='5b12d53a-3108-470b-b055-167cc3276594', ...)]\n",
      "E-1001 (BAD_COLUMN_NAME): Glyph refers to nonexistent column name: fill_color [renderer: GlyphRenderer(id='5b12d53a-3108-470b-b055-167cc3276594', ...)]\n",
      "W-1004 (BOTH_CHILD_AND_ROOT): Models should not be a document root if they are in a layout box: Figure(id='1a17b0b6-3eeb-4514-8a3a-5d25534ea160', ...)\n",
      "W-1004 (BOTH_CHILD_AND_ROOT): Models should not be a document root if they are in a layout box: Figure(id='20e33fe2-7b7a-4695-8692-0a151d2930f9', ...)\n"
     ]
    }
   ],
   "source": [
    "l=layout ([\n",
    "    [plot],\n",
    "    [algo_attribute_select],\n",
    "    [p]\n",
    "])\n",
    "curdoc().add_root(l)\n",
    "show(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
